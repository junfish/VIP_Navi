# NaVIP: An Image-Centric Indoor <ins>Navi</ins>gation Solution for <ins>V</ins>isually <ins>I</ins>mpaired <ins>P</ins>eople

[Jun Yu](https://scholar.google.com/citations?user=fh1iSyAAAAAJ&hl=en), Yifan Zhang, Badrinadh Aila, [Vinod Namboodiri](https://engineering.lehigh.edu/faculty/vinod-namboodiri)

by ACCESS Lab at the Lehigh University

### To-Do List

- [x] **April 14th, 2024**: training & inference code for the baseline models---PoseNet, Bayesian PoseNet, LSTM-PoseNet, Learnable PoseNet, Geometric PoseNet, Hourglass PoseNet, BranchNet-Euler6, etc.
- [x] **July 6th, 2024**: utils for data preprocessing, annotation, and gpt-4 inquiry are made publicly available.
- [ ] **July 17th, 2024**: our iOS application iNaVIP are shared with research community, including both server (Python based) and client (Swift-based) code.
- [ ] ......
- [ ] ......

### A Short Introduction

This project aims to develop an image-centric application for accessible and inclusive indoor navigation. Our focus is on addressing the most challenging scenario: individuals who are visually impaired and unable to see their surroundings. From this starting point, the application will infer their location, guide them to their destinations, and inform them about their surroundings. GLHF. All of us!

